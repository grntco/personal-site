---
slug: thinking-fast-and-slow
title: Thinking, Fast and Slow
date: 2025-8-27
---

These are my notes from reading _Thinking, Fast and Slow_ by Daniel Kahneman.

**System 1 and System 2.** Throughout the book, Kahneman's describes two systems of human thinking. He points out that there are not actually two systems in our brains, but that we can better understand human behavior and judgement as the the result of these two systems at play. System 1 (fast thinking) is our intuitive, lazy thinking that happens naturally without having to put in any effort. System 2 (slow thinking), on the other hand, requires effort and logic.

**WYSIATI.** What You See Is All There Is is essentially jumping to conclusions without all the evidence. It's a System 1 process that takes the already known facts and ideas, and makes sense of them by generating a coherent narrative around them. It doesn't allow for new information. It's one-sided and optimizes for cognitive ease.

**Substituting questions.** When presented with a complex question, System 1 excels at substituting a different, simpler question and answering that instead. For example, questions like "How much would you contribute to save an endangered species?" or "How happy are you with your life these days?" are complex questions, so System 1 substitutes easier questions like "How much emotion do I feel when I think of dolphins?" and "What is my mood right now?" instead.

**Law of small numbers.** Small sample sizes are unreliable and can be unrepresentative of the larger population. In small samples, the results are often due to chance. However, System 1, using WYSIATI, jumps in and quickly creates a narrative around the results, searching for causes which might not be really be there.

**Anchoring.** When we are presented a question with a number, we use that number as a reference point. That number becomes the anchor. For example, in negotiating the cost of a car, you will base your evaluations off the first number (anchor) offerred.

**Availability heuristic.** Similar to WYSIATI. We overestimate that which we see. Media coverage warps disaster availability: rare negative events (i.e., plane crashes, mass shootings, etc.) broadcast to us on social media or by the news cause us to think these things happen more frequently than they actually do.

On this note, Kahneman describes a study in which people were presented with two causes of death that they then had to rate as more and less likely. Death by accidents (a frequently covered subject in the media) was generally rated as much more probable than diseases and conditions that have, in reality, much higher death tolls: strokes, asthma, botulism, and even diabetes.

**If you have one wish for your child, wish them to be an optimist.** Perhaps unfortunately, however, optimism has largely been found to be genetic. Being an optimist is a general predictor of well-being. Optimists have less depression and stronger immune systems, take better care of their health and live longer, and are more resilient, cheerful, and popular than pessimists.

Most entrepreneurs are optimists. In markets that are extremely competitive, optimistic entrepreneurs don't think the high rates of failure apply to them. You have to be optimistic to win in these conditions. Similarly, risk takers don't necessarily love the risk, they might just be less aware of it than timid people.

**Rare events and denominator neglect.** 0.1% reads differently than 1 in 1000. We tend to devalue the denominator in the fraction, causing that option to appear more likely. Also, when paired with vivid imagery, rare events seem more probable (see the availability heuristic and media coverage). Kahneman describes the results of one study:

> "People who saw information about 'a disease that kills 1,286 people out of every 10,000' judged it as more dangerous than people who were told about 'a disease that kills 24.14% of the population.' The first disease appears more threatening than the second, although the former risk is only half as large as the latter!" (p. 329).

**We avoid actions that admit failure.** We steer clear of actions that will lead to regret. It's the sunk cost fallacy. We struggle to give up and move on from doomed projects and situations. The money you have in a bad investment is likely not worth staying in that investment with the hopes of it going in the green again. Admit failure, take the loss, sell, and use that money in a better investment.

**Frame-bound vs. reality-bound preferences.** Consider how things are framed. We would rather avoid loss than achieve gains. Kahneman presents two sentences: "The one-month survival rate is 90%." and "There is a 10% mortality rate." Even though the sentences are logically the same, a 90% rate of survival seems pretty good whereas a 10% death rate seems awful. (p. 367). Our System 1 lazily just accepts how situations are framed without employing System 2 and reconsidering an alternate view.

**The experiencing self vs. the remembering self.** The remembering self creates stories about what happened that it can replay. The remembering self takes over from the experiencing self, and it often skews toward whatever quality the final moments of an experience had. For example, just because you remember the last bad part of an experience doesn't mean the experience was all bad.

Speaking of memories, Kahneman describes how time is ignored in replaying memories. The stories we create, even if they are about experiences that happened over long stretches of real time, don't feel that way in our memories.

**The focusing illusion:** nothing in life is as important as you think it is when you are thinking about it. When people are asked to evaluate their lives, they are reminded of good or bad things, rather than relating their current experience. Instead of the question, "How much do you like your car?", the real question is “How much pleasure do you get from your car _when you think about it_?”

**Can psychology be taught?** As interesting as the above ideas are, does it actually help to have this knowledge? In the final chapter, Kahneman writes, "System 1 is not readily educable." Even Kahneman, who conducted many of the experiments validating these ideas, falls prey to the common misjudgments of our intuitive thinking. I'm sure it's possible to train your mind to avoid some of the pitfalls of System 1 mentioned earlier, but most of the time these things just happen automatically and without us having an opportunity to catch our ourselves before it's too late.

## Conclusions

Honestly, much of this book was over my head, particularly later chapters about Bernoulli's errors, prospect theory, and a lot of the probability-weighted decision-making stuff. Still, I'm glad I read it for learning about, thinking about, and, hopefully, applying some of the above lessons.

I read one review (on Goodreads, I believe) that said you can just read this book and not waste time reading other popular books on behavioral psychology. I haven't read enough books on the topic to know if this is accurate, but I presume (at least my System 1 and WYSIATI jumps to the conclusion) that this is probably a fair assessment.

As an aside, while I was reading this I listened to an older episode of Lex Fridman's podcast where he interviewed Kahneman about these ideas as they related to building AI systems, which was pretty interesting to think about. This was before ChatGPT and other LLMs, too.
